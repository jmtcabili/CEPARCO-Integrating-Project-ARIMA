{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONFEqFtvSnDA1xz2+znXeE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmtcabili/CEPARCO-Integrating-Project-ARIMA/blob/main/Autoressive_Coefficient_Calculation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4TB6nRzvpFp",
        "outputId": "63f5c3c4-25cf-4300-84ba-3c91c94c0b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lagged.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile lagged.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__\n",
        "void autoregressive(size_t n, float *lagged, float *in, int lagged_cols)\n",
        "{\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y; // Row index\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x; // Column index\n",
        "\n",
        "    int rowStride = blockDim.y * gridDim.y;\n",
        "    int colStride = blockDim.x * gridDim.x;\n",
        "\n",
        "    for (int i = row; i < n; i+= rowStride){\n",
        "      for (int j = col; j < lagged_cols; j+= colStride){\n",
        "        if (j == 0){\n",
        "          lagged[i * lagged_cols + j] = 1;\n",
        "        }else if (i < n && j < lagged_cols) {\n",
        "            int index = i - j;\n",
        "            if (index < 0) {\n",
        "                lagged[i * lagged_cols + j] = 0; // Assign zero for out-of-bounds indices\n",
        "            } else {\n",
        "                lagged[i * lagged_cols + j] = in[index]; // Assign lagged value\n",
        "            }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "__global__\n",
        "void transpose(float *out, float *in, int p, size_t ARRAY_SIZE){\n",
        "\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "  if (row < p && col < ARRAY_SIZE) {\n",
        "      out[row * ARRAY_SIZE + col] = in[col * p + row];\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "__global__\n",
        "void matMulNaive(float *dest, float *in1, float *in2,\n",
        "                 size_t in1_height, size_t in2_height,\n",
        "                 size_t in1_width, size_t in2_width)\n",
        "{\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Each thread computes one element of the result matrix\n",
        "    float cValue = 0;\n",
        "\n",
        "    if (row < in1_height && col < in2_width) {\n",
        "        // Matrix multiplication: in1 (lagged_cols x n) * in2 (n x lagged_cols)\n",
        "        for (int k = 0; k < in1_width; ++k) {\n",
        "            cValue += in1[row * in1_width + k] * in2[k * in2_width + col];\n",
        "        }\n",
        "        dest[row * in2_width + col] = cValue;\n",
        "    }\n",
        "\n",
        "    //p+1 n x n p+1 -> first mul = p+1 mat\n",
        "    //p+1 p+1 x p+1 n -> second mul = p+1 x n\n",
        "}\n",
        "\n",
        "\n",
        "//Matrix inverse functions\n",
        "__global__ void nodiag_normalize(float *A, float *I, int n, int i){\n",
        "\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (x < n && y < n)\n",
        "\tif (x == i && x!=y){\n",
        "\t\tI[x*n + y] /= A[i*n + i];\n",
        "\t\tA[x*n + y] /= A[i*n + i];\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "__global__ void diag_normalize(float *A, float *I, int n, int i){\n",
        "\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (x < n && y < n)\n",
        "\tif (x == y && x == i){\n",
        "\t\tI[x*n + y] /= A[i*n + i];\n",
        "\t\tA[x*n + y] /= A[i*n + i];\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "__global__ void gaussjordan(float *A, float *I, int n, int i)\n",
        "{\n",
        "\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (x < n && y < n){\n",
        "\t\tif (x != i){\n",
        "\t\t\tI[x*n + y] -= I[i*n + y] * A[x*n + i];\n",
        "\t\t\tif (y != i){\n",
        "\t\t\t\tA[x*n + y] -= A[i*n + y] * A[x*n + i];\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "__global__ void set_zero(float *A, float *I, int n, int i){\n",
        "\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\tif (x < n && y < n){\n",
        "\t\tif (x != i){\n",
        "\t\t\tif (y == i){\n",
        "\t\t\t\tA[x*n + y] = 0;\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "int main (){\n",
        "\n",
        "  //dataset\n",
        "  const size_t ARRAY_SIZE = 10;\n",
        "  const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "  //const size_t numOfLoops = 30;\n",
        "\n",
        "\n",
        "  //arima parameters and variables;\n",
        "\n",
        "  const int p = 4;\n",
        "  const int lagged_cols = p + 1;\n",
        "  const int q = 1;\n",
        "\n",
        "  const size_t AR_SIZE = p + 1;\n",
        "  const size_t AR_BYTES = AR_SIZE * sizeof(float);\n",
        "  const size_t X_BYTES = ARRAY_SIZE * lagged_cols * sizeof(float);\n",
        "  const size_t PXP_BYTES = AR_SIZE * AR_SIZE * sizeof(float);\n",
        "\n",
        "  // declare arrays\n",
        "  float *in, *out, *lagged, *transposed, *transposed2,\n",
        "        *prod1, *inverse, *identity, *prod2, *AR_COEFF;\n",
        "  cudaMallocManaged(&in, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&out, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&lagged, X_BYTES); //same amount of rows and p cols\n",
        "  cudaMallocManaged(&transposed,X_BYTES);\n",
        "  cudaMallocManaged(&transposed2,X_BYTES);\n",
        "  cudaMallocManaged(&prod2,X_BYTES);\n",
        "  cudaMallocManaged(&prod1,PXP_BYTES);\n",
        "  cudaMallocManaged(&inverse,PXP_BYTES);\n",
        "  cudaMallocManaged(&identity,PXP_BYTES);\n",
        "  cudaMallocManaged(&AR_COEFF, AR_BYTES);\n",
        "\n",
        "\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);  //get GPU id\n",
        "\n",
        "  // memory advise\n",
        "  cudaMemAdvise(in, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(in, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "\n",
        "  cudaMemPrefetchAsync(in,ARRAY_BYTES,cudaCpuDeviceId,NULL);                       //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(lagged,ARRAY_SIZE * lagged_cols * sizeof(float),device,NULL);         //\"prefetch data\" to create GPU page memory\n",
        "\n",
        "  for (size_t i=0; i<ARRAY_SIZE; i++)\n",
        "      in[i] = i % 5 + 1.0;\n",
        "\n",
        "  cudaMemPrefetchAsync(in,ARRAY_BYTES,device,NULL);                                //prefetch from CPU to GPU\n",
        "\n",
        "  dim3 threadsPerBlock(16, 16);\n",
        "  dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x-1)/threadsPerBlock.x,\n",
        "                 (lagged_cols + threadsPerBlock.y-1)/threadsPerBlock.y);\n",
        "\n",
        "  autoregressive<<<numBlocks, threadsPerBlock>>> (ARRAY_SIZE,lagged, in, lagged_cols);\n",
        "  cudaGetLastError();\n",
        "\n",
        "  // synchronize GPU with CPU\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // prefetch from GPU to CPU\n",
        "  cudaMemPrefetchAsync(lagged,ARRAY_SIZE * lagged_cols * sizeof(float),cudaCpuDeviceId,NULL);\n",
        "\n",
        "\n",
        "  for (int i = ARRAY_SIZE-10; i < ARRAY_SIZE; i++){\n",
        "    for (int j = 0; j < lagged_cols; j++){\n",
        "      printf(\"%.2f \", lagged[i*lagged_cols+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "  //---------- Transposing lagged matrix ----------------//\n",
        "  //try removing later along with prev prefetch async and print\n",
        "\n",
        "\n",
        "  cudaMemAdvise(lagged, X_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(lagged, X_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "\n",
        "  cudaMemPrefetchAsync(lagged,X_BYTES,cudaCpuDeviceId,NULL);                       //\"prefetch data\" to create CPU page memory\n",
        "\n",
        "  cudaMemPrefetchAsync(transposed,X_BYTES, device, NULL);                                   //\"prefetch data\" to create GPU page memory\n",
        "\n",
        "  cudaMemPrefetchAsync(lagged, X_BYTES, device, NULL);\n",
        "\n",
        "\n",
        "  //dim3 dimGrid(ARRAY_SIZE/TILE_DIM, p/TILE_DIM, 1);\n",
        "  //dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);\n",
        "\n",
        "  transpose<<<numBlocks, threadsPerBlock>>>(transposed, lagged, lagged_cols, ARRAY_SIZE);\n",
        "  cudaGetLastError();\n",
        "\n",
        "  // synchronize GPU with CPU\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // prefetch from GPU to CPU\n",
        "  cudaMemPrefetchAsync(transposed,X_BYTES,cudaCpuDeviceId,NULL);\n",
        "\n",
        "\n",
        "  //---printing tranposed---//\n",
        "  printf(\"\\n\");\n",
        "\n",
        "  for (int i = 0; i < lagged_cols; i++){\n",
        "    for (int j = 0; j < 10; j++){\n",
        "      printf(\"%.2f \", transposed[i*ARRAY_SIZE+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  cudaMemPrefetchAsync(prod1, PXP_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(lagged, X_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(transposed, X_BYTES, device, NULL);\n",
        "\n",
        "  matMulNaive<<<numBlocks, threadsPerBlock>>>(prod1, transposed, lagged,\n",
        "                                              lagged_cols, ARRAY_SIZE, ARRAY_SIZE, lagged_cols);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  if (err != cudaSuccess) {\n",
        "      printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "\n",
        "  // Prefetch result back to CPU\n",
        "  cudaMemPrefetchAsync(prod1, PXP_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  // Print results if needed\n",
        "  printf(\"\\nMatrix multiplication result:\\n\");\n",
        "  for (int i = 0; i < lagged_cols; i++) {\n",
        "      for (int j = 0; j < lagged_cols; j++) {\n",
        "          printf(\"%.2f \", prod1[i * lagged_cols + j]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  dim3 threadsPerBlockInv(lagged_cols, lagged_cols);\n",
        "  dim3 numBlocksInv((lagged_cols + lagged_cols -1) / lagged_cols,\n",
        "                 (lagged_cols+lagged_cols-1)/lagged_cols);\n",
        "\n",
        "  cudaMemPrefetchAsync(inverse, PXP_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(prod1, PXP_BYTES, device, NULL);\n",
        "\n",
        "  //set identity matrix\n",
        "  for (int i = 0; i < lagged_cols; i++){\n",
        "    for (int j = 0; j < lagged_cols; j++){\n",
        "      if (i == j)\n",
        "        inverse[i * lagged_cols + j] = 1.0;\n",
        "      else\n",
        "        inverse[i * lagged_cols + j] = 0.0;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"\\nIdentity Matrix result:\\n\");\n",
        "  for (int i = 0; i < lagged_cols; i++) {\n",
        "      for (int j = 0; j < lagged_cols; j++) {\n",
        "          printf(\"%.2f \", inverse[i * lagged_cols + j]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "\n",
        "  for (int i = 0; i < lagged_cols; i++){\n",
        "    nodiag_normalize <<<numBlocksInv, threadsPerBlockInv >>>(prod1, inverse, lagged_cols, i);\n",
        "    cudaDeviceSynchronize();\n",
        "\t\tdiag_normalize <<<numBlocksInv, threadsPerBlockInv>>>(prod1, inverse, lagged_cols, i);\n",
        "    cudaDeviceSynchronize();\n",
        "\t\tgaussjordan <<<numBlocksInv, threadsPerBlockInv>>>(prod1, inverse,lagged_cols, i);\n",
        "    cudaDeviceSynchronize();\n",
        "\t\tset_zero <<<numBlocksInv, threadsPerBlockInv>>>(prod1, inverse, lagged_cols, i);\n",
        "    cudaDeviceSynchronize();\n",
        "  }\n",
        "\n",
        "  // Prefetch result back to CPU\n",
        "  cudaMemPrefetchAsync(inverse, PXP_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  printf(\"\\nInverse result:\\n\");\n",
        "  for (int i = 0; i < lagged_cols; i++) {\n",
        "      for (int j = 0; j < lagged_cols; j++) {\n",
        "          printf(\"%.9f \", inverse[i * lagged_cols + j]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  cudaMemPrefetchAsync(transposed, X_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(inverse, PXP_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(prod2, X_BYTES, device, NULL);\n",
        "\n",
        "  matMulNaive<<<numBlocks, threadsPerBlock>>>(prod2, inverse, transposed, lagged_cols, lagged_cols, lagged_cols, ARRAY_SIZE);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "  err = cudaGetLastError();\n",
        "  if (err != cudaSuccess) {\n",
        "      printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "\n",
        "  // Prefetch result back to CPU\n",
        "  cudaMemPrefetchAsync(prod2, X_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  printf(\"\\n\");\n",
        "  // Print results if needed\n",
        "  for (int i = 0; i < lagged_cols; i++){\n",
        "    for (int j = 0; j < 10; j++){\n",
        "      printf(\"%.9f \", prod2[i*ARRAY_SIZE+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  cudaMemPrefetchAsync(prod2, X_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(AR_COEFF, AR_BYTES, device, NULL);\n",
        "\n",
        "  matMulNaive<<<numBlocks, threadsPerBlock>>>(AR_COEFF, prod2, in, lagged_cols, ARRAY_SIZE, ARRAY_SIZE, 1);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "  err = cudaGetLastError();\n",
        "  if (err != cudaSuccess) {\n",
        "      printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "\n",
        "  // Prefetch result back to CPU\n",
        "  cudaMemPrefetchAsync(AR_COEFF, X_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  for (int i = 0; i < lagged_cols; i++){\n",
        "    printf(\"%.5f\\n\", AR_COEFF[i]);\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  //cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(in);\n",
        "  cudaFree(out);\n",
        "  cudaFree(lagged);\n",
        "  cudaFree(transposed);\n",
        "  cudaFree(prod1);\n",
        "  cudaFree(prod2);\n",
        "  cudaFree(inverse);\n",
        "\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -arch=sm_75 lagged.cu -o lagged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAh2Z7mSwT9u",
        "outputId": "fa3886b5-4851-4862-b5d3-b9d7ff418a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mlagged.cu(134)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q\"\u001b[0m was declared but never referenced\n",
            "    const int q = 1;\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./lagged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DUn0oIdwVJg",
        "outputId": "ae914827-78e7-4b92-aca5-44972b49d8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==6122== NVPROF is profiling process 6122, command: ./lagged\n",
            "1.00 0.00 0.00 0.00 0.00 \n",
            "1.00 1.00 0.00 0.00 0.00 \n",
            "1.00 2.00 1.00 0.00 0.00 \n",
            "1.00 3.00 2.00 1.00 0.00 \n",
            "1.00 4.00 3.00 2.00 1.00 \n",
            "1.00 5.00 4.00 3.00 2.00 \n",
            "1.00 1.00 5.00 4.00 3.00 \n",
            "1.00 2.00 1.00 5.00 4.00 \n",
            "1.00 3.00 2.00 1.00 5.00 \n",
            "1.00 4.00 3.00 2.00 1.00 \n",
            "\n",
            "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
            "0.00 1.00 2.00 3.00 4.00 5.00 1.00 2.00 3.00 4.00 \n",
            "0.00 0.00 1.00 2.00 3.00 4.00 5.00 1.00 2.00 3.00 \n",
            "0.00 0.00 0.00 1.00 2.00 3.00 4.00 5.00 1.00 2.00 \n",
            "0.00 0.00 0.00 0.00 1.00 2.00 3.00 4.00 5.00 1.00 \n",
            "\n",
            "Matrix multiplication result:\n",
            "10.00 25.00 21.00 18.00 16.00 \n",
            "25.00 85.00 65.00 51.00 44.00 \n",
            "21.00 65.00 69.00 53.00 43.00 \n",
            "18.00 51.00 53.00 60.00 47.00 \n",
            "16.00 44.00 43.00 47.00 56.00 \n",
            "\n",
            "Identity Matrix result:\n",
            "1.00 0.00 0.00 0.00 0.00 \n",
            "0.00 1.00 0.00 0.00 0.00 \n",
            "0.00 0.00 1.00 0.00 0.00 \n",
            "0.00 0.00 0.00 1.00 0.00 \n",
            "0.00 0.00 0.00 0.00 1.00 \n",
            "\n",
            "Inverse result:\n",
            "0.439719111 -0.092227302 -0.016625991 -0.020980148 -0.022794995 \n",
            "-0.092227288 0.062221933 -0.034666050 0.006436187 -0.001321231 \n",
            "-0.016626017 -0.034666050 0.079890825 -0.038300183 0.002787953 \n",
            "-0.020980153 0.006436189 -0.038300179 0.080425262 -0.037153382 \n",
            "-0.022794986 -0.001321232 0.002787946 -0.037153378 0.054449663 \n",
            "\n",
            "0.439719111 0.347491801 0.238638535 0.108805075 -0.043823361 -0.196451813 0.112056270 0.042557821 -0.005169902 -0.043823361 \n",
            "-0.092227288 -0.030005354 -0.002449472 0.031542595 0.064213425 0.096884280 -0.181554556 0.024446538 0.024936439 0.064213425 \n",
            "-0.016626017 -0.051292069 -0.006067291 0.000857297 0.010569846 0.020282395 0.203325182 -0.186416402 0.014797062 0.010569846 \n",
            "-0.020980153 -0.014543965 -0.046407953 0.002153315 0.013561208 0.024969094 0.004196052 0.207104847 -0.183613598 0.013561208 \n",
            "-0.022794986 -0.024116218 -0.022649504 -0.058336169 -0.039573170 -0.020810172 0.004558980 0.009382248 0.213912144 -0.039573170 \n",
            "1.98226\n",
            "0.51558\n",
            "-0.10141\n",
            "-0.12485\n",
            "0.10405\n",
            "==6122== Profiling application: ./lagged\n",
            "==6122== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   31.26%  158.94us         1  158.94us  158.94us  158.94us  autoregressive(unsigned long, float*, float*, int)\n",
            "                   27.52%  139.93us         3  46.644us  4.1910us  83.423us  matMulNaive(float*, float*, float*, unsigned long, unsigned long, unsigned long, unsigned long)\n",
            "                   18.29%  93.023us         5  18.604us  3.7440us  77.055us  nodiag_normalize(float*, float*, int, int)\n",
            "                   14.27%  72.543us         1  72.543us  72.543us  72.543us  transpose(float*, float*, int, unsigned long)\n",
            "                    3.17%  16.096us         5  3.2190us  3.0080us  3.3600us  gaussjordan(float*, float*, int, int)\n",
            "                    3.01%  15.328us         5  3.0650us  3.0080us  3.1360us  diag_normalize(float*, float*, int, int)\n",
            "                    2.49%  12.640us         5  2.5280us  2.4960us  2.5600us  set_zero(float*, float*, int, int)\n",
            "      API calls:   98.53%  124.45ms        10  12.445ms  3.4610us  124.39ms  cudaMallocManaged\n",
            "                    0.48%  606.21us        25  24.248us  5.9980us  162.00us  cudaDeviceSynchronize\n",
            "                    0.37%  462.35us        25  18.493us  3.5580us  284.17us  cudaLaunchKernel\n",
            "                    0.34%  431.22us        22  19.601us     829ns  69.784us  cudaMemPrefetchAsync\n",
            "                    0.16%  201.68us       114  1.7690us     163ns  83.584us  cuDeviceGetAttribute\n",
            "                    0.07%  84.748us         7  12.106us  6.8520us  30.634us  cudaFree\n",
            "                    0.03%  34.701us         4  8.6750us  2.3140us  24.151us  cudaMemAdvise\n",
            "                    0.01%  14.752us         1  14.752us  14.752us  14.752us  cuDeviceGetName\n",
            "                    0.01%  8.1380us         1  8.1380us  8.1380us  8.1380us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0490us         1  2.0490us  2.0490us  2.0490us  cudaGetDevice\n",
            "                    0.00%  1.7950us         5     359ns     265ns     548ns  cudaGetLastError\n",
            "                    0.00%  1.3860us         3     462ns     168ns     927ns  cuDeviceGetCount\n",
            "                    0.00%  1.2430us         2     621ns     190ns  1.0530us  cuDeviceGet\n",
            "                    0.00%     663ns         1     663ns     663ns     663ns  cuModuleGetLoadingMode\n",
            "                    0.00%     445ns         1     445ns     445ns     445ns  cuDeviceTotalMem\n",
            "                    0.00%     282ns         1     282ns     282ns     282ns  cuDeviceGetUuid\n",
            "\n",
            "==6122== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "       3  4.0000KB  4.0000KB  4.0000KB  12.00000KB  9.536000us  Host To Device\n",
            "       6  4.0000KB  4.0000KB  4.0000KB  24.00000KB  11.04000us  Device To Host\n",
            "       6         -         -         -           -  422.5270us  Gpu page fault groups\n",
            "       2  4.0000KB  4.0000KB  4.0000KB  8.000000KB           -  Memory thrashes\n",
            "Total CPU Page faults: 3\n",
            "Total CPU thrashes: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}